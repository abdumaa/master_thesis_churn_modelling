{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting GBT candidate(s)\n",
    "\n",
    "This notebook contains core functions used for fitting the GBT candidates. The code behind these functions is in the [lgbm.py](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/lgbm.py) file.\n",
    "The cells below are executed by clicking on them and pressing \"shift+enter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust path!\n",
    "\n",
    "Please make sure to execute the cell below. This will adjust your current path. Additionally \"SCRIPT_DIR\" is later used to cache the [fits](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/playground/lgbm_fits) and [results](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/playground/lgbm_results) in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR[:-11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Execute one M-HPTL and get&save the best fit\n",
    "\n",
    "The following cells will run you through one Model-Hyperparameter-Tuning-Loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import the LGBM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_modelling.modelling.lgbm import LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Call the class\n",
    "\n",
    "The class automatically loads all datasets from the [data](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/data) folder as [attributes](https://github.com/abdumaa/master_thesis_churn_modelling/blob/14077d754b48e962f494e3eb4b335f6b88945f6f/churn_modelling/modelling/lgbm.py#L39)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_modelling = LGBM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Seperate training data from validation data\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/14077d754b48e962f494e3eb4b335f6b88945f6f/churn_modelling/modelling/lgbm.py#L70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = gbt_modelling.create_train_val()\n",
    "print(f\"Length of training set: {len(df_train)}\\nLength of validation set: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Sample data\n",
    "\n",
    "Supported parameters for sampling: \"up\", \"down\", \"smote\" \\\n",
    "Supported parameters for frac: a float which is larger than 0 \\\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/14077d754b48e962f494e3eb4b335f6b88945f6f/churn_modelling/modelling/lgbm.py#L84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sampled = gbt_modelling.create_sampling(\n",
    "    df_to_sample=df_train,\n",
    "    sampling=\"up\",\n",
    "    frac=10,\n",
    ")\n",
    "print(df_train_sampled[\"churn\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Detect the best set of features\n",
    "\n",
    "This function returns the best set of features. It performs [MRMR](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/preprocessing/mrmr.py) on the quotation variables to reduce redundant variables. \\\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/lgbm.py#L113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feats = gbt_modelling.get_best_quot_features(\n",
    "    df_to_dimreduce=df_train_sampled,\n",
    "    cv=5,\n",
    "    return_fix_features=True,\n",
    "    return_target=True\n",
    ")\n",
    "print(f\"Best set of features: {best_feats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Define Hyperparameter dictionaries\n",
    "\n",
    "These dictionaries will be used for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "# Define fix hyperparameters passed into the lightgbm.Classifier\n",
    "hp_fix_dict = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"max_depth\": -1,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"importance_type\": \"split\",\n",
    "}\n",
    "# Define the to be tuned hyperparameters and their value spaces for GBT\n",
    "hp_tune_dict = {\n",
    "    \"num_leaves\": sp_randint(6, 50),\n",
    "    \"min_child_weight\": [1e-5, 1e-2, 1e-1, 1, 1e1, 1e4],\n",
    "    \"min_child_samples\": sp_randint(100, 500),\n",
    "    \"subsample\": sp_uniform(loc=0.4, scale=0.6),\n",
    "    \"colsample_bytree\": sp_uniform(loc=0.6, scale=0.4),\n",
    "    \"reg_alpha\": [0, 1, 5, 10, 100],\n",
    "    \"reg_lambda\": [0, 1, 5, 10, 100],\n",
    "}\n",
    "# Define parameters regarding evaluation and early stoping\n",
    "### Note that 'eval_metric' is automatically overwritten when a custom-loss is used ###\n",
    "hp_eval_dict = {\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"callbacks\": [lgb.log_evaluation(100), lgb.early_stopping(30)],\n",
    "}\n",
    "# Define parameters regarding the tuning process\n",
    "rscv_params = {\n",
    "    \"n_iter\": 100,\n",
    "    \"n_jobs\": -1,\n",
    "    \"cv\": 3,\n",
    "    \"verbose\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Fit models\n",
    "\n",
    "By defining \"cl_alpha\" and \"cl_gamma\" other than \"None\", [Weighted Loss](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/custom_loss.py#L86) or [Focal Loss](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/custom_loss.py#L6) are used as objective loss and evalution functions. \\\n",
    "The model is saved as \"cache_model_name\" in [lgbm_fits](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/playground/lgbm_fits). \\\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/lgbm.py#L164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_fit = gbt_modelling.fit_lgbm(\n",
    "    df_train=df_train_sampled,\n",
    "    df_val=df_val,\n",
    "    hp_fix_dict=hp_fix_dict,\n",
    "    hp_tune_dict=hp_tune_dict,\n",
    "    hp_eval_dict=hp_eval_dict,\n",
    "    rscv_params=rscv_params,\n",
    "    feature_set=best_feats,\n",
    "    cl_alpha=None,\n",
    "    cl_gamma=None,\n",
    "    save_model=True,\n",
    "    cache_model_name=\"lgbm_fit_gbt_up1_best_quot_aNone_gNone\",\n",
    "    path_to_folder=SCRIPT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Predict on OOS data using fit\n",
    "\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/lgbm.py#L307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, preds_proba = gbt_modelling.predict(\n",
    "    gbt_modelling.df_oos,\n",
    "    predict_from_cached_fit=False,\n",
    "    fit=gbt_fit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "print('Accuracy_OOS:', round(accuracy_score(gbt_modelling.df_oos[\"churn\"], preds), 4),\n",
    "'\\nPrecision_OOS:', round(precision_score(gbt_modelling.df_oos[\"churn\"], preds), 4),\n",
    "'\\nRecall_OOS:', round(recall_score(gbt_modelling.df_oos[\"churn\"], preds), 4),\n",
    "'\\nF1_Score_OOS:', round(f1_score(gbt_modelling.df_oos[\"churn\"], preds), 4),\n",
    "'\\nAUROC_OOS:', round(roc_auc_score(gbt_modelling.df_oos[\"churn\"], preds_proba), 4),\n",
    "'\\nAUPRC_OOS:', round(average_precision_score(gbt_modelling.df_oos[\"churn\"], preds_proba), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execute entire S-HPTL and get&save the best fits\n",
    "\n",
    "The following cells perform the Structural-Hyperparameter-Tuning-Loop. Note that M-HPTL nests in S-HPTL, so in each iteration of S-HPTL the entire M-HPTL is executed. This increases the computational time drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Hyperparameter dictionaries\n",
    "\n",
    "These dictionaries will be used for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "# Define fix hyperparameters passed into the lightgbm.Classifier\n",
    "hp_fix_dict = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"max_depth\": -1,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"importance_type\": \"split\",\n",
    "}\n",
    "# Define the to be tuned hyperparameters and their value spaces for GBT\n",
    "hp_tune_dict = {\n",
    "    \"num_leaves\": sp_randint(6, 50),\n",
    "    \"min_child_weight\": [1e-5, 1e-2, 1e-1, 1, 1e1, 1e4],\n",
    "    \"min_child_samples\": sp_randint(100, 500),\n",
    "    \"subsample\": sp_uniform(loc=0.4, scale=0.6),\n",
    "    \"colsample_bytree\": sp_uniform(loc=0.6, scale=0.4),\n",
    "    \"reg_alpha\": [0, 1, 5, 10, 100],\n",
    "    \"reg_lambda\": [0, 1, 5, 10, 100],\n",
    "}\n",
    "# Define parameters regarding evaluation and early stoping\n",
    "### Note that 'eval_metric' is automatically overwritten when a custom-loss is used ###\n",
    "hp_eval_dict = {\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"callbacks\": [lgb.log_evaluation(100), lgb.early_stopping(30)],\n",
    "}\n",
    "# Define parameters regarding the tuning process\n",
    "rscv_params = {\n",
    "    \"n_iter\": 100,\n",
    "    \"n_jobs\": -1,\n",
    "    \"cv\": 3,\n",
    "    \"verbose\": 100,\n",
    "}\n",
    "# Define hyperparameter spaces for S-HPTL\n",
    "# The function expects the same presented structure\n",
    "# Note that by adding more values the computational time increases drastically\n",
    "hp_struct_dict = {\n",
    "    'sampling': {\n",
    "        'down1': 0.1,\n",
    "        'down2': 0.5,\n",
    "        # 'down3': x,\n",
    "        # 'up1': y,\n",
    "        # 'smote': True,\n",
    "    },\n",
    "    'dr_method': ['no_quot', 'best_quot'],\n",
    "    'cl_alpha': [None, 0.6],\n",
    "    'cl_gamma': [None],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fit and Evaluate models\n",
    "\n",
    "The best models of each S-HPTL iteration are saved in [lgbm_fits](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/playground/lgbm_fits). \\\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/lgbm.py#L380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = gbt_modelling.fit_and_eval_lgbm_candidates(\n",
    "    hp_struct_dict=hp_struct_dict,\n",
    "    hp_fix_dict=hp_fix_dict,\n",
    "    hp_tune_dict=hp_tune_dict,\n",
    "    hp_eval_dict=hp_eval_dict,\n",
    "    rscv_params=rscv_params,\n",
    "    path_to_folder=SCRIPT_DIR,\n",
    "    feature_set_from_last_fits=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Review results\n",
    "\n",
    "This table is also saved in [lgbm_results](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/modelling/lgbm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python39164bit1b7085399b144131b3c6aab0c8fc3c91"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
