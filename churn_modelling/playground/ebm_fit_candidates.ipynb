{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting EBM candidate(s)\n",
    "\n",
    "This notebook contains core functions used for fitting the EBM candidates. The code behind these functions is in the [ebm.py](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/ebm.py) file.\n",
    "The cells below are executed by clicking on them and pressing \"shift+enter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust path!\n",
    "\n",
    "Please make sure to execute the cell below. This will adjust your current path. Additionally \"SCRIPT_DIR\" is later used to cache the [fits](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/playground/ebm_fits) and [results](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/playground/ebm_results) in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR[:-11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Execute one M-HPTL and get&save the best fit\n",
    "\n",
    "The following cells will run you through one Model-Hyperparameter-Tuning-Loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import the EBM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_modelling.modelling.ebm import EBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Call the class\n",
    "\n",
    "The class automatically loads all datasets from the [data](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/data) folder as [attributes](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/ebm.py#L37)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_modelling = EBM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sample data\n",
    "\n",
    "Supported parameters for sampling: \"up\", \"down\", \"smote\" \\\n",
    "Supported parameters for frac: a float which is larger than 0 \\\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/ebm.py#L68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sampled = ebm_modelling.create_sampling(\n",
    "    df_to_sample=ebm_modelling.df,\n",
    "    sampling=\"down\",\n",
    "    frac=0.5,\n",
    ")\n",
    "print(df_train_sampled[\"churn\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Detect the best set of features\n",
    "\n",
    "This function returns the best set of features. It performs [MRMR](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/preprocessing/mrmr.py) on the quotation variables to reduce redundant variables. \\\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/ebm.py#L97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feats = ebm_modelling.get_best_quot_features(\n",
    "    df_to_dimreduce=df_train_sampled,\n",
    "    cv=5,\n",
    "    return_fix_features=True,\n",
    "    return_target=True\n",
    ")\n",
    "print(f\"Best set of features: {best_feats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Define Hyperparameter dictionaries\n",
    "\n",
    "These dictionaries will be used for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "# Define fix hyperparameters passed into the ebm.Classifier\n",
    "hp_fix_dict = {\n",
    "    \"validation_size\": 0.1111, # to achieve 80/20/20\n",
    "    \"early_stopping_rounds\": 30,\n",
    "    \"early_stopping_tolerance\": 1e-4,\n",
    "    \"max_rounds\": 5000,\n",
    "}\n",
    "# Define the to be tuned hyperparameters and their value spaces for EBM\n",
    "hp_tune_dict = {\n",
    "    \"interactions\": sp_randint(5, 10),\n",
    "    \"outer_bags\": sp_randint(10, 20), # computationally very costly\n",
    "    \"inner_bags\": sp_randint(0, 10), # computationally very costly\n",
    "    \"learning_rate\": sp_uniform(loc=0.009, scale=0.006),\n",
    "    \"min_samples_leaf\": sp_randint(2, 5),\n",
    "    \"max_leaves\": sp_randint(2, 5),\n",
    "}\n",
    "# Define parameters regarding the tuning process\n",
    "rscv_params = {\n",
    "    \"n_iter\": 10,\n",
    "    \"n_jobs\": -1,\n",
    "    \"cv\": 3,\n",
    "    \"verbose\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Fit models\n",
    "\n",
    "By defining \"cl_alpha\" and \"cl_gamma\" other than \"None\", [Weighted Loss](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/custom_loss.py#L86) or [Focal Loss](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/custom_loss.py#L6) are used as objective loss and evalution functions. \\\n",
    "The model is saved as \"cache_model_name\" in [ebm_fits](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/playground/ebm_fits). \\\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/ebm.py#L148)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_fit = ebm_modelling.fit_ebm(\n",
    "    df_train=df_train_sampled,\n",
    "    hp_fix_dict=hp_fix_dict,\n",
    "    hp_tune_dict=hp_tune_dict,\n",
    "    rscv_params=rscv_params,\n",
    "    feature_set=best_feats,\n",
    "    save_model=True,\n",
    "    cache_model_name=\"test\",\n",
    "    path_to_folder=SCRIPT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Predict on OOS data using fit\n",
    "\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/ebm.py#L231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, preds_proba = ebm_modelling.predict(\n",
    "    ebm_modelling.df_oos,\n",
    "    predict_from_cached_fit=False,\n",
    "    fit=ebm_fit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "print('Accuracy_OOS:', round(accuracy_score(ebm_modelling.df_oos[\"churn\"], preds), 4),\n",
    "'\\nPrecision_OOS:', round(precision_score(ebm_modelling.df_oos[\"churn\"], preds), 4),\n",
    "'\\nRecall_OOS:', round(recall_score(ebm_modelling.df_oos[\"churn\"], preds), 4),\n",
    "'\\nF1_Score_OOS:', round(f1_score(ebm_modelling.df_oos[\"churn\"], preds), 4),\n",
    "'\\nAUROC_OOS:', round(roc_auc_score(ebm_modelling.df_oos[\"churn\"], preds_proba), 4),\n",
    "'\\nAUPRC_OOS:', round(average_precision_score(ebm_modelling.df_oos[\"churn\"], preds_proba), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execute entire S-HPTL and get&save the best fits\n",
    "\n",
    "The following cells perform the Structural-Hyperparameter-Tuning-Loop. Note that M-HPTL nests in S-HPTL, so in each iteration of S-HPTL the entire M-HPTL is executed. This increases the computational time drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define Hyperparameter dictionaries\n",
    "\n",
    "These dictionaries will be used for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "# Define fix hyperparameters passed into the ebm.Classifier\n",
    "hp_fix_dict = {\n",
    "    \"validation_size\": 0.1111,\n",
    "    \"early_stopping_rounds\": 30,\n",
    "    \"early_stopping_tolerance\": 1e-4,\n",
    "    \"max_rounds\": 5000,\n",
    "}\n",
    "# Define the to be tuned hyperparameters and their value spaces for EBM\n",
    "hp_tune_dict = {\n",
    "    \"interactions\": sp_randint(5, 10),\n",
    "    \"outer_bags\": sp_randint(10, 20), # computationally very costly\n",
    "    \"inner_bags\": sp_randint(0, 10), # computationally very costly\n",
    "    \"learning_rate\": sp_uniform(loc=0.009, scale=0.006),\n",
    "    \"min_samples_leaf\": sp_randint(2, 5),\n",
    "    \"max_leaves\": sp_randint(2, 5),\n",
    "}\n",
    "# Define parameters regarding the tuning process\n",
    "rscv_params = {\n",
    "    \"n_iter\": 10,\n",
    "    \"n_jobs\": -1,\n",
    "    \"cv\": 3,\n",
    "    \"verbose\": 100,\n",
    "}\n",
    "# Define hyperparameter spaces for S-HPTL\n",
    "# The function expects the same presented structure\n",
    "# Note that by adding more values the computational time increases drastically\n",
    "hp_struct_dict = {\n",
    "    'sampling': {\n",
    "        'down1': 0.1,\n",
    "        'down2': 0.5,\n",
    "        # 'down3': x,\n",
    "        # 'up1': y,\n",
    "        # 'smote': True,\n",
    "    },\n",
    "    'dr_method': ['no_quot', 'best_quot'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fit and Evaluate models\n",
    "\n",
    "The best models of each S-HPTL iteration are saved in [ebm_fits](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/playground/ebm_fits). \\\n",
    "[code](https://github.com/abdumaa/master_thesis_churn_modelling/blob/main/churn_modelling/modelling/ebm.py#L296)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = ebm_modelling.fit_and_eval_ebm_candidates(\n",
    "    hp_struct_dict=hp_struct_dict,\n",
    "    hp_fix_dict=hp_fix_dict,\n",
    "    hp_tune_dict=hp_tune_dict,\n",
    "    rscv_params=rscv_params,\n",
    "    path_to_folder=SCRIPT_DIR,\n",
    "    feature_set_from_last_fits=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Review results\n",
    "\n",
    "This table is also saved in [lgbm_results](https://github.com/abdumaa/master_thesis_churn_modelling/tree/main/churn_modelling/modelling/ebm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python39164bit1b7085399b144131b3c6aab0c8fc3c91"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
