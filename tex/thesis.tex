\documentclass[12pt,titlepage]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage[a4paper,lmargin={3cm},rmargin={2cm},tmargin={2cm},bmargin={2cm}]{geometry}
\usepackage[onehalfspacing]{setspace}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[pdftex]{graphicx}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{trees}
\graphicspath{{./images/}}


\begin{document}

\thispagestyle{empty}

\begin{titlepage}\centering
    \begin{center}
        \vspace*{\fill}
        \huge \textbf{\textsf{Customer Churn Prediction using Quotation Data}}\\
        \vspace{2cm}
        \LARGE\textbf{\textsc{Master Thesis}}\\
        \vspace{1cm}
        \normalsize
        Submitted on: \today \\
        \vspace{2.5cm}
        \large \textbf{at the University of Cologne}
        \vspace{3cm}
    \end{center}
    \normalsize{
        \begin{tabular}{ll}
            Name: & {Abdurahman Maarouf} \\
            Adress: & {Schulstrasse 31} \\
            Postcode, Area: & {53332, Bornheim} \\
            Country: & {Germany} \\
            Matriculation number: & {736481} \\
            Supervisor: & {Prof. Dr. Dominik Wied} \\
        \end{tabular}\\
    }
    \vspace*{\fill}

\end{titlepage}

\thispagestyle{empty}

\tableofcontents

\newpage

\pagenumbering{arabic}

\setcounter{page}{1}

\section{Introduction} \par

Predicting customer churn in order to retain customers has become one of the most important issues for companies.
The goal is to estimate probabilities for a costumer churning in the next period of time, in order to be able to detect
potential churners before they leave the company. To tackle this issue, more and more advanced Machine-Learning-Algorithms
are used guaranteeing high accuracy in their out-of-sample predictions. \\
Fortunately for most of the companies, churn rates from one period to another are very small. However in classification
models predicting a rare event can become challenging. In this so called "Imballanced Classes" issue certain arrangements
to the underlying training data need be made. Without these arrangements and with highly imballanced classes, a poor
algorithm will simply never predict the outcome of the minority class. In a dataset with 1000 customers containing 5
churners for example, this loss-minimizing algorithm would have an in-sample accuracy of 99.5\%. \\
In order to avoid the high amount of "False-Negative" classifications there are many methods ranging from upsampling the
minority class or downsampling the majority class to more advanced techniques. In this work we will present and compare
the different methods while applying them to the underlying problem. \\
We also want to emphasize (or not) the importance of using quotation data for predicting customer churn. A company can
track (potential) customer behavior on their distribution channels. Nowadays, in most cases the products or services are
offered online on websites, which makes it easy to track website visitor data. In the context of dealing with customer
churn this data can be matched to the customers already having a product or contract of this company. We believe (?) that
the number of visits of a current customer in the last period (?) plays a big role in predicting the probability of that
customer leaving in the next period. (Coming from high correlation between Nvisits and churn)\\
In order to evaluate the importance of not only the number of website visits but also the other explanatory variables there
is typically a trade-off during model selection. The trade-off is between the model complexity or corresponding accuracy
and the model interpretability. Deep neural networks or boosted trees belong to the complex models which are famous for
their high accuracy in the fields of computer vision and natural language processing. Understanding and interpreting the
model is of no big interest in these areas. However in the topic of this work and in many other areas understanding which
variables lead to the resulting outcome of the model becomes desirable. The most transparent models in terms of
interpretability are linear or logistic models. There the magnitude and sign of the corresponding coefficients (after being
testet for significance) illustrate the changes of the outcome for a change in the specific explanatory variable. These models
however lack in terms of accuracy when being compared to the comlex ones. In this work we will present the accuracy and
interpretability of "Explainable Boosting Machines" developped by (?) for predicting customer churn. It aims to combine the
high accuracy of complex models on the one hand and the interpretability of linear models on the other hand. \\

\section{Data and Methodology} \par

\subsection{Understanding the Problem} \par

For this work we use customer data from a big insurance company in Germany. Due to data pretection the data is anonymized which
does not affect the model accuracy and interpretability in any form. We focus on the product of automobile liability insurance,
which is by law a mandatory service every car owner must hold in Germany. \\
Typically car owners close a deal with an insurance company which can be terminated by the end of each year. In rare cases both
sides agree on a contract with a due date during the year. If the contract does not get terminated it is automatically extended
for another year. Besides the option to terminate the contract at the due date there is also an option to terminate
it earlier in a few special cases. These cases mainly involve car accidents and vehicle changes of the contractor. To sum up,
here are the three cases in which a churn can (but not must) occur during a year: \\

\begin{center}
    \begin{tabular}{ll}
        Event A: & Contractor is involved in an Accident. \\
        Event N: & Contractor buys a new Car. \\
        Event D: & Due date during the year. \\
    \end{tabular}
\end{center}

The problem of modelling customer churn needs to be seperated into the probability of a costumer leaving during the
year and at the end of a year (why noch ausf√ºhren). In this work we will focus on predicting churns occuring during the year.
The purpose is to build a model which can be used at any time $t$ of the year besides on January the 1st to predict the probability of
a costumer leaving the company in the next period of time $(t, t+s]$. \\
It can be argued that in order to provide a model with maximized utility for production one would want to keep $s$ small. For example
a company would highly benifit from a model, which can predict the churn-probability of tomorrow or the next week. However we will see that
having a small $s$ will decrease the accuracy of our models (drastically?), creating a trade-off situation between model accuracy and the benifits
of a small $s$. With a smaller period the classes of the data become more imballanced, creating a higher challange of preprocessing the training
data and feeding the algorithm enough information on potential churners. Furthermore, a small $s$ decreases the scope of action for a company to
retain potential customers leaving. \\

Figure 1 (richtiger Verweis) illustrates how the probability of a churn during the year can be decomposed using the Events A (Accident), N
(New Car), D (Due date during the year) and C (Churn). \\

% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=5cm, sibling distance=3cm]
\tikzstyle{level 2}=[level distance=5cm, sibling distance=2cm]

% Define styles for bags and leafs
\tikzstyle{bag} = [text width=4em, text centered]
\tikzstyle{end} = [circle, minimum width=3pt,fill, inner sep=0pt]

% The sloped option gives rotated edge labels.
\begin{tikzpicture}[grow=right, sloped]
\node[bag] {}
    child {
        node[bag] {$None$}
            child {
                node[end, label=right:
                    {$P(None\cap \overline{Churn})$}] {}
                edge from parent
                node[above] {$P(\overline{C}\mid None)$}
            }
            child {
                node[end, label=right:
                    {$P(None\cap Churn)$}] {}
                edge from parent
                node[above] {$P(C\mid None)$}
            }
            edge from parent
            node[above] {$P(None)$}
    }
    child {
        node[bag] {$Due$ $Date$}
        child {
                node[end, label=right:
                    {$P(Due$ $Date\cap \overline{Churn})$}] {}
                edge from parent
                node[above] {$P(\overline{C}\mid D)$}
            }
            child {
                node[end, label=right:
                    {$P(Due$ $Date\cap Churn)$}] {}
                edge from parent
                node[above] {$P(C\mid D)$}
            }
        edge from parent
            node[above] {$P(D)$}
    }
    child {
        node[bag] {$New$ $Car$}
        child {
                node[end, label=right:
                    {$P(New$ $Car\cap \overline{Churn})$}] {}
                edge from parent
                node[above] {$P(\overline{C}\mid N)$}
            }
            child {
                node[end, label=right:
                    {$P(New$ $Car\cap Churn)$}] {}
                edge from parent
                node[above] {$P(C\mid N)$}
            }
        edge from parent
            node[above] {$P(N)$}
    }
    child {
        node[bag] {$Accident$}
        child {
                node[end, label=right:
                    {$P(Accident\cap \overline{Churn})$}] {}
                edge from parent
                node[above] {$P(\overline{C}\mid A)$}
            }
            child {
                node[end, label=right:
                    {$P(Accident\cap Churn)$}] {}
                edge from parent
                node[above] {$P(C\mid A)$}
            }
        edge from parent
            node[above] {$P(A)$}
    };
\end{tikzpicture} \par

By assumption we set $P(C\mid None) = 0$ as the amount of terminated contracts during the year which are not being
caused by a new car, an accident or a due date is very small and can be omitted. Therefore we leave these cases out of our data
(?). Also, the probability $P(D)$ can only take values $0$ and $1$, as either the due date of a customer lies in the next period
of time $(t, t+s]$ or not. What we are interested in predicting is the overall probability of a churn, which can be rewritten as:
(Change this to a formula with a number next to it?)

\begin{center}
    \begin{tabular}{ll}
        $P(C)$ & $=$ \hspace{3mm} $P(A\cap C) + P(N\cap C) + P(D\cap C)$ \\
        & $=$ \hspace{3mm} $P(A)P(C\mid A) + P(N)P(C\mid N) + P(D)P(C\mid D)$ \\
    \end{tabular}
\end{center} \par

One idea would be to model the three branches of $A$, $N$ and $D$ seperately. The logic behind this is that different models and
sets of explanatory variables have the best fit for the probabilities of the three branches. Not only the the three branches
but also the unconditional and conditional probabilities of a single branch may vary in their best modelling approach. For prediciting
an accident a model of type (XY) is more suitable, whereas (YZ) would go better with modelling the probability of churn given an
accident occured. In the course of this work we will begin with a general modelling approach trying to predict the probability $P(C)$.
At a later stage we will compare the accuracy outcomes of seperately predicting the branch probabilities with the baseline approach. \\

\subsection{Data} \par

Most customer churn predicition models have been using only one timestamp to train a model. We want to emphasize the usage of multiple timestamps and
show that it significantly improves prediction accuracy. The improvement is a result of an increased training set size and the ability of the model to
especially learn more about the rare class. Furthermore, the model becomes more generalizable in time which is crucial if this model is applied in
production. (see: How training on multiple time slices improves performance in churn prediction). Explain more. Use Graphics like in the paper.\\
Part of this work will also be to evaluate if the churn probability of customers is time invariant. Therefore we statistically test the equality of
monthly and yearly (and weekly?) mean churn rates using the ANOVA (or other? seasonality tests). We will see that it is time variant (or not?) which underlines the
importance of including (or not) different timestamps containing different years/months(/weeks) in the data for the model to learn the time differences
(or not). (These will simply be appended to the data as additional rows creating a panel dataset.) \\
Therefore to build the models we use historical data of the insurance company. More specifically we pick $N$ (one or many?) timestamp(s) $t_{i}$ with
$i = 1,...,N$ in the past and collect all the active contracts to that(these) timestamp(s). One row corresponds to one active contract at $t_{i}$. So
if one hypothetical contract is active in all $N$ periods it will appear as $N$ seperate rows in our data. To each row we merge the status of that
contract in $t_{i}+s$. Furthermore, we join the number of requests corresponding to that contract in the period $[t_{i}-m, t_{i}]$. The period length $m$
will be another parameter to tune. \\
Illustrate graphically like in Gattermann? \\
Describe the ETL Process? \\

-Statt "STORNO" im data-load sql query zu definieren, definieren wir es einfach in dem Feature-Engineering Teil?\\
-Test if customer churn probability is time invariant or time variant!!! Is it enough to argue with the Aggregate Churn rate? Show statistically (Voll gute Idee)\\
-How do I exclude 1.1. churners in my data? Exclude the entire contract or simply say y=0 as he didnt churn during the year?\\
-Safe noch als Variable einbauen, was sein Preis bei einer jetzigen Berechnung w√§re (und dann Differenz zwischen Preis, den er zahlt und Preis den er angezeigt bekommt!)\\

\section{Modelling Approach} \par

\subsection{Literature Review} \par

see Gattermann et al. 2. Literature Review to get an idea on how to do this Literature Review. \\
Put an introduction in to the next subsections. Say sth like "there is a wide range of literature on classification techniques and specifically on customer churn predicition.
Baseline: Logistic Regression. They propose following advanced techniques. In chapter 3.4 we are going to focus on X, Y and Z" \\
Also include introductory words for the other subsections. \\

\subsection{Preprocessing} \par

Put the ETL process here? Including multiple time stamps explanation here instead of Chapter 2? \\

\subsection{Handling Class Imbalance} \par

Studying the rarity of an event in the context of machine learning has become an important challange in the recent two decades. Rare events, such as a customer churning in the
next period, are much harder to identify and learn for most of the models. HaiYing Wang et al (Logistic Regression for Massive Data with Rare Events) study the convergence rate
and distributaional properties of a Maximum-Likelihood estimator for the parameters of a logistic regression while prediciting rare events. Their finding is that the convergence
rate of the MLE is equal to the inverse of the number of examples in the minority class rather then the overall size of the training data set. So the accuracy of the estimates for
the parameters is limited to the available information on the minority class, even if the size of the dataset is massive. \\
Therefore some methods have been developped to decrease the problematic of imballanced classes. In
this chapter we will present (three?) different methods which can be applied to the training set, before feeding it to the model. To handle and evaluate the outcomes
of prediciting rare events also the appropriate models and model evaluation metrics must be chosen. This will be discussed in the next two chapters. \\

\subsubsection*{(i) Downsampling}

The first basic sampling method is named downsampling. It randomly eliminates examples from the majority class in order to artifficially decrease the imballance between the
two classes. The downside of this approach is that it possibly eliminates useful examples for the model to maintain a high accuracy in predicting the majority class 
(Mining with Rarity: A Unifying Framework). HaiYing Wang et al (Logistic Regression for Massive Data with Rare Events) also study the convergence rate and distributaional
properties when applying downsampling. According to their findings the asymptotic distribution of the resulting parameters may be identical to the MLE's using the full data set.
Under this condition there is no loss in terms of efficiency (minimum possible variance of an unbiased estimator devided by its actual variance). \\

\subsubsection*{(ii) Upsampling}

The second basic sampling method is the upsampling approach. This method simply duplicates examples from the minority class until the classes are more balanced.
While duplicating examples though, the chances of overfitting to these duplicates becomes a more probable threat.
Also, no new data is being generated in order to let the model learn more valuable characteristics about the minority class (Mining with Rarity: A Unifying Framework).
Additionally, the computational performance of this approach can get rather poor, espacially with large datasets and highly imballanced classes. While evaluating the
asymptotics of the MLE's with upsampling, HaiYing Wang et al find out that it also decreases the efficiency. A probable higher asymptotic variance of the estimators
is the reason for that. \\


\subsubsection*{(iii) SMOTE}

The more advanced SMOTE-approach (Synthetic Minority Oversampling Technique) (SMOTE: Synthetic Minority Over-sampling Technique) also creates more artifficial examples of
the minority class. Instead of simply duplicating some rows SMOTE creates new nearest neighbors in terms of feature values for the minority class examples. 
While constructing the feature values of the new example $(n + 1)$ (where n is the size of the unsampled dataset) as a new nearest neighbor for example $i$ of the
minority class one has to differentiate between continuous and nominal features. The k-nearest neighbors for the minority class are typically constructed with the Euclidean
Distance for continuous features and the Value Distance Metric for nominal features (maybe include other distance measures here? Mahalanobis Distance?).\\
A. Continuous features: \\
A.1) Construct difference between corresponding feature value of example $i$ and one of its k nearest neighbors. \\
A.2) Multiply this difference with a random value drawn from a uniform distribution between 0 and 1. \\
A.3) Construct the feature value of the new example by adding the multiplied difference to the feature value of example $i$. \\
B. Nominal features: \\
B.1) Choose the feature value which is the majority vote between the feature value $i$ and its k nearest neighbors. \\
B.2) Assign this value to the corresponding feature of the new example. \\
With this approach it is ensured that the model learns more about the neighborhood regions of the minority class. It decreases the probability, that the model overfits to the
duplicates created in upsampling. \\

\subsubsection*{(iv) Cost-Sensitive Classifiers} \label{Cost-Sensitive Classifiers}
One drawback of the presented sampling methods is that the sample distribution is being changed. A different starting point is to change the objective cost function which
is to be minimized during the model estimation. Thereby the sample size and the distribution stay the same. In the modified cost function we want to penalize false-negative
classifications with a higher weight. Reason behind this is to ensure that the event of interest, represented by the minority class, is predicted correctly. \\
(How does our cost-function look like then? Gattermann use balanced class weights which are inversely proportional to the class frequency and thereby assign more weight to samples
from the minority class.) (or cite: Cost-sensitive learning methods for imbalanced data)  \\
The challange of this approach is to find the appropriate penalty weight. It is hard to measure the cost of misclassifying an insurance-customer regarding churn-probabilities.
The fact that these costs can come from multiple sources which are not easily definable is one part of the reasoning. Therefore one conventional method is to assign inverse of the class frequency to the associated weights in the cost function. (So the to be minimized cost function would look like this:)\\

\subsection{Machine Learning Models and Interpretability} \par

\subsubsection*{(i) Logisitic Regression}
The logistic regression is used as a benchmark model for all classification problems. It has high advantages in terms of computational complexity and interpretability,
but fails in capturing complex relationships and interactions. As in most cases it has the lowest accuracy of all classifiers we use it as a baseline model in order to
evaluate the advantages of additional complexity in the models, as in Random Forests, Boosted Trees and Explainable Boosting Machines. Thereby we focus on model performance on unseen data and model interpretability.\\
Logisitic regression applies a sigmoid function to the input characteristics in order to get values (probabilities) between $0$ and $1$ as a Bernoulli distribution. We will use the following conventional notation throughout this work: \\

\begin{equation} \label{bern_dist}
    \widehat{C_{t+s}} = P(C_{t+s}=1\mid X_{t}) = f\big(g(X_{t})\big) = \frac{e^{g(X_{t})}}{1 + e^{g(X_{t})}} \\
\end{equation}

Note that from now on we use a vectorized notation. So $\hat{C}$ represents a vector of probabilities, where the i'th element ($i = 1,...,N$) corresponds to the probability of a churn in time $t+s$ of the same at $t$ active contract. This probability vector is constructed by a model conditional on a characteristic matrix $X$ at time $t$. Again, each row of $X$ represents a different contract and column j is the characteristic j ($j = 1,...,M$) of that contract. \\
Therefore $f(\cdot)$ is the sigmoid function used on function $g(\cdot)$, where in logistic regression $g(\cdot)$ is a simple linear model. For Explainable Boosting Machines, this function is allowed to be more complex. \\
Maximum Likelihood is then used to estimate the vector of parameters $\theta$ of the model. It maximizes the joint probability that the status of the training data contracts are drawn from the Bernoulli distribution stated in (\ref{bern_dist}). For the final model in production, when estimating probabilities for a churn in the next period, $t$ is of course equal across all contracts in $X$. But as already stated, for training the model we use multiple timestamps of the historized data. Therefore the index $t$ is left out for the input matrix $X$ and output vector $C$ while estimating model parameters, avoiding a misleading notation. Still, the status of contract $i$ is determined after $s$ units of time for all contracts. The likelihood function to be maximized is described as follows: \\

\begin{equation} \label{logistic_MLE}
    L(C; X, \theta) = \prod_{i=1}^{N}P\big(C^{(i)}=1\mid X^{(i)}\big)^{I(C^{(i)}=1)}\Big(1 - P\big(C^{(i)}=1\mid X^{(i)}\big)^{1 - I(C^{(i)}=1)}\Big) \\
\end{equation}

Then the loss function is characterized by the negative log of (\ref{logistic_MLE}), which can be minimized with respect to $\theta$ using Gradient Descent, Newton Raphson (Zitieren!) or other optimization proceedings.

\begin{equation} \label{logistic_log_MLE}
    \begin{aligned}
        l(C; X, \theta) & = -\sum_{i=1}^{N}C^{(i)}log\big(P(C^{(i)}=1\mid X^{(i)})\big) + (1 - C^{(i)})log\big(1 - P(C^{(i)}=1\mid  X^{(i)})\big) \\
        & = -\sum_{i=1}^{N}C^{(i)}log(\widehat{C^{(i)}}) + (1 - C^{(i)})log(1 - \widehat{C^{(i)}}) \\
    \end{aligned}
\end{equation}

In our case we have a lot (how many?) of characteristics in the raw matrix $X$. Consequently it makes sense to add a penalty term to the loss function to avoid overfitting to the training data. (Wenn zu wenig geschrieben mehr auf Overfittingproblem eingehen?) Additionally, as already mentioned in (\ref{Cost-Sensitive Classifiers}), one technique to handle imbalanced classes is to use cost-sensitive classifiers. We can implement this by setting additional penalty weights to our loss function. \\

\begin{equation} \label{loss_fct}
    l(C; X, \theta) = -\sum_{i=1}^{N}w_{1}C^{(i)}log(\widehat{C^{(i)}}) + w_{0}(1 - C^{(i)})log(1 - \widehat{C^{(i)}}) + \lambda h(\theta)
\end{equation}

Here, the penalty term $h(\cdot)$ can either be an L1 or L2 regularization on $\theta$. The additional parameters of $\lambda$, $w_{0}$ and $w_{1}$ are hyperparameters which can be tuned during grid search. Following (whose?) approach, $w_{1}$ and $w_{0}$ can also be set by default to the inverse of their corresponding class frequencies in the training data. \\
After having found the optimal set of parameters it is time to define the classification cutoff $\tau$ for predicting a churn. The logistic regression model outputs a probability which is between 0 and 1. The rule of thumb is to set $\tau = 0.5$, such that probabilities larger than $0.5$ are predicted as churns and the rest as non-churns. \\
Interpretability. \\

\subsubsection*{(i) Random Forest Classifier}

Classification trees have a different approach on builing a prediction model for $\widehat{C_{t+s}} = P(C_{t+s}=1\mid X_{t})$. They search for the optimal sequential binary sample splits in order to minimize an objective loss function. So at each node of the tree the optimal characteristic $j$ and its optimal split point $r$ need to be found. The search at each node can be summarized as follows:

\begin{equation} \label{dec_tree}
    \begin{aligned}
        S_{1}(j, r) = \{X\mid X^{j}\geq r\} , S_{2}(j, r) = \{X\mid X^{j}< r\} \\
        \{j, r\} \in \min_{j, s} \sum_{i:X^{(i)}\in S_{1}(j, r)}l(C^{(i)}; X^{(i)}, \widehat{C^{(i)}}) + \sum_{i:X^{(i)}\in S_{2}(j, r)}l(C^{(i)}; X^{(i)}, \widehat{C^{(i)}})
    \end{aligned}
\end{equation}

The loss function for region $S_{k}$ is calculated using the predictions $\widehat{C^{(i)}}$ ($i:X^{(i)}\in S_{k}$), which are simply the shares of churns in $S_{k}$. The typical loss functions are based on evaluating the purity of the resulting regions. In the ideal case, one would like to find the splits in $X$ which always correctly assign contracts of the two classes into completely different regions. In this case, $\widehat{C^{(i)}}$ would always be either 1 or 0. In order approach this case one either uses the Gini-index or the Cross-entropy for region $S_{k}$: \\

\begin{center}
    \begin{tabular}{ll}
        Gini-index: & $2\widehat{C^{(i)}}(1-\widehat{C^{(i)}})$ \\
        Cross-entropy: & $-\widehat{C^{(i)}}log(\widehat{C^{(i)}}) - (1-\widehat{C^{(i)}})log(1-\widehat{C^{(i)}})$ \\
    \end{tabular}
\end{center}

\subsection{Model Evaluation Metrics} \par

\newpage

\thispagestyle{empty}

\section*{Bibliography}
\vspace*{6mm}

\end{document}s